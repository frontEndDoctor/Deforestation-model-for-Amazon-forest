{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#To import the required libraries\nimport numpy as np \nimport pandas as pd \nimport os\nimport gc\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\n#To import some features of the tensorflow framework\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom sklearn.metrics import fbeta_score\nfrom tqdm import tqdm\nimport cv2\nfrom PIL import Image\n\n#To import some features of Keras that I'll be using later on\nfrom tensorflow import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D, BatchNormalization\nfrom keras.optimizers import Adam, RMSprop\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras import optimizers\nfrom keras import backend as K\n\n#for model training, I'll import the required libraries from scikitlearn\nfrom sklearn.model_selection import KFold, train_test_split\nfrom sklearn.metrics import fbeta_score\nimport time\n","execution_count":3,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#next step is to load and read the dataset using pandas\nforest_train =pd.read_csv('/kaggle/input/planets-dataset/planet/planet/train_classes.csv')\nforest_test =pd.read_csv('../input/planets-dataset/planet/planet/sample_submission.csv')","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"70"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#let's see the categories or placeholders in  the dataset\nflatten = lambda l: [item for sublist in l for item in sublist]\nlabels_map = list(set(flatten([l.split(' ') for l in forest_train['tags'].values])))\nprint(labels_map)","execution_count":7,"outputs":[{"output_type":"stream","text":"['slash_burn', 'water', 'agriculture', 'cultivation', 'cloudy', 'artisinal_mine', 'blow_down', 'bare_ground', 'primary', 'habitation', 'selective_logging', 'road', 'conventional_mine', 'partly_cloudy', 'haze', 'blooming', 'clear']\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":8,"outputs":[{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"20"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#assigning numerical values to each label in the form pf dictionary\nlabels_map = {i:j for j, i in enumerate(labels_map)}\nlabels_map","execution_count":9,"outputs":[{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"{'slash_burn': 0,\n 'water': 1,\n 'agriculture': 2,\n 'cultivation': 3,\n 'cloudy': 4,\n 'artisinal_mine': 5,\n 'blow_down': 6,\n 'bare_ground': 7,\n 'primary': 8,\n 'habitation': 9,\n 'selective_logging': 10,\n 'road': 11,\n 'conventional_mine': 12,\n 'partly_cloudy': 13,\n 'haze': 14,\n 'blooming': 15,\n 'clear': 16}"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":10,"outputs":[{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"40"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The next step is encoding of the train and test sets respectively\n\n#Like I always do, I'll begin with the train set\n\n# But first, let's convert the images into pixels and resize them to save memory space\n\nX_train, Y_train = [], []\nfor img, label in tqdm(forest_train.values, miniters = 1000):\n    target = np.zeros(17)\n    for tag in label.split(' '):\n        target[labels_map[tag]]=1\n    X_train.append(cv2.resize(cv2.imread('../input/planets-dataset/planet/planet/train-jpg/{}.jpg'.format(img)), (64,64)))\n    Y_train.append(target)","execution_count":11,"outputs":[{"output_type":"stream","text":"100%|██████████| 40479/40479 [01:51<00:00, 362.45it/s]\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":13,"outputs":[{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"20"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#The same procedure will be applied on the test set\n\nX_test=[]\nfor img, label in tqdm(forest_test[:40669].values, miniters = 1000):\n  X_test.append(cv2.resize(cv2.imread('../input/planets-dataset/planet/planet/test-jpg/{}.jpg'.format(img)), (64,64)))\nfor img, label in tqdm(forest_test[40669:].values, miniters = 1000):\n  X_test.append(cv2.resize(cv2.imread('../input/planets-dataset/test-jpg-additional/test-jpg-additional/{}.jpg'.format(img)), (64,64)))","execution_count":14,"outputs":[{"output_type":"stream","text":"100%|██████████| 40669/40669 [02:13<00:00, 303.93it/s]\n100%|██████████| 20522/20522 [01:06<00:00, 307.29it/s]\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":15,"outputs":[{"output_type":"execute_result","execution_count":15,"data":{"text/plain":"40"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Change lists to numpy arrays and normalize\n\nx_train = np.array(X_train, np.float16)/255\ny_train = np.array(Y_train, np.uint8)\n\nx_test = np.array(X_test, np.float16)/255\n\nx_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size = 0.2, shuffle = True, random_state = 1)\n\nprint(x_train.shape, y_train.shape, x_val.shape, y_val.shape,x_test.shape)","execution_count":16,"outputs":[{"output_type":"stream","text":"(32383, 64, 64, 3) (32383, 17) (8096, 64, 64, 3) (8096, 17) (61191, 64, 64, 3)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":17,"outputs":[{"output_type":"execute_result","execution_count":17,"data":{"text/plain":"40"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Therefore, the CNN model here will have 5 layers as earlier stated by the K-folds\n\n#Save the weights in the paths created thus;\n\nkfold_weights_path = os.path.join('', 'weights_kfold_' + '.h5')\n\n\ncnn_model = Sequential()\n\n#first layer\ncnn_model.add(BatchNormalization(input_shape=(64, 64,3)))\ncnn_model.add(Conv2D(32, kernel_size=(3, 3),padding='same', activation='relu'))\ncnn_model.add(Conv2D(32, (3, 3), activation='relu'))\ncnn_model.add(MaxPooling2D(pool_size=(2, 2)))\ncnn_model.add(Dropout(0.25))\n\n#second layer\ncnn_model.add(Conv2D(64, kernel_size=(3, 3),padding='same', activation='relu'))\ncnn_model.add(Conv2D(64, (3, 3), activation='relu'))\ncnn_model.add(MaxPooling2D(pool_size=(2, 2)))\ncnn_model.add(Dropout(0.25))\n\n#third layer\ncnn_model.add(Conv2D(128, kernel_size=(3, 3),padding='same', activation='relu'))\ncnn_model.add(Conv2D(128, (3, 3), activation='relu'))\ncnn_model.add(MaxPooling2D(pool_size=(2, 2)))\ncnn_model.add(Dropout(0.25))\n\n#fourth layer\ncnn_model.add(Conv2D(256, kernel_size=(3, 3),padding='same', activation='relu'))\ncnn_model.add(Conv2D(256, (3, 3), activation='relu'))\ncnn_model.add(MaxPooling2D(pool_size=(2, 2)))\ncnn_model.add(Dropout(0.25))\n \n#fifth layer    \ncnn_model.add(Flatten())\ncnn_model.add(Dense(512, activation='relu'))\ncnn_model.add(BatchNormalization())\ncnn_model.add(Dropout(0.5))\ncnn_model.add(Dense(17, activation='sigmoid'))\n\n#defining the metrics to use\ndef fbeta(y_true, y_pred, threshold_shift=0):\n    beta = 2\n\n    # just in case of hipster activation at the final layer\n    y_pred = K.clip(y_pred, 0, 1)\n\n        # shifting the prediction threshold from .5 if needed\n    y_pred_bin = K.round(y_pred + threshold_shift)\n\n    tp = K.sum(K.round(y_true * y_pred_bin)) + K.epsilon()\n    fp = K.sum(K.round(K.clip(y_pred_bin - y_true, 0, 1)))\n    fn = K.sum(K.round(K.clip(y_true - y_pred, 0, 1)))\n\n    precision = tp / (tp + fp)\n    recall = tp / (tp + fn)\n\n    beta_squared = beta ** 2\n    return (beta_squared + 1) * (precision * recall) / (beta_squared * precision + recall + K.epsilon())\n    \n\n#Adding some optimization methods to the model to prevent overfitting\nepochs = 20\nlearn_rate = 0.0001\nopt  = optimizers.Adam(lr=learn_rate)\ncnn_model.compile(loss='binary_crossentropy',optimizer=opt,metrics=[fbeta])\ncallbacks = [EarlyStopping(monitor='val_loss', patience=2, verbose=0)]\n\n#Each of the above models will be checked while the one with the best weighed will be saved\ncnn_model.fit( x_train,  y_train, validation_data=(x_val, y_val),batch_size=128,verbose=2, epochs=epochs,callbacks=callbacks,shuffle=True)\n","execution_count":18,"outputs":[{"output_type":"stream","text":"Epoch 1/20\n253/253 - 8s - loss: 0.6223 - fbeta: 0.6190 - val_loss: 0.5443 - val_fbeta: 0.6110\nEpoch 2/20\n253/253 - 7s - loss: 0.3857 - fbeta: 0.7434 - val_loss: 0.2978 - val_fbeta: 0.6424\nEpoch 3/20\n253/253 - 7s - loss: 0.2267 - fbeta: 0.7503 - val_loss: 0.1991 - val_fbeta: 0.7187\nEpoch 4/20\n253/253 - 7s - loss: 0.1789 - fbeta: 0.7575 - val_loss: 0.1712 - val_fbeta: 0.7528\nEpoch 5/20\n253/253 - 7s - loss: 0.1620 - fbeta: 0.7675 - val_loss: 0.1652 - val_fbeta: 0.7503\nEpoch 6/20\n253/253 - 7s - loss: 0.1522 - fbeta: 0.7784 - val_loss: 0.1593 - val_fbeta: 0.7428\nEpoch 7/20\n253/253 - 7s - loss: 0.1466 - fbeta: 0.7845 - val_loss: 0.1515 - val_fbeta: 0.7687\nEpoch 8/20\n253/253 - 7s - loss: 0.1417 - fbeta: 0.7925 - val_loss: 0.1429 - val_fbeta: 0.7859\nEpoch 9/20\n253/253 - 7s - loss: 0.1389 - fbeta: 0.7963 - val_loss: 0.1383 - val_fbeta: 0.7848\nEpoch 10/20\n253/253 - 7s - loss: 0.1356 - fbeta: 0.8006 - val_loss: 0.1337 - val_fbeta: 0.7925\nEpoch 11/20\n253/253 - 7s - loss: 0.1335 - fbeta: 0.8052 - val_loss: 0.1296 - val_fbeta: 0.8075\nEpoch 12/20\n253/253 - 7s - loss: 0.1305 - fbeta: 0.8098 - val_loss: 0.1341 - val_fbeta: 0.7888\nEpoch 13/20\n253/253 - 7s - loss: 0.1289 - fbeta: 0.8135 - val_loss: 0.1318 - val_fbeta: 0.7919\n","name":"stdout"},{"output_type":"execute_result","execution_count":18,"data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7f68f4031890>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":19,"outputs":[{"output_type":"execute_result","execution_count":19,"data":{"text/plain":"1553"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#to save the training predictions\n\n\nyfull_test=[]\n\np_val = cnn_model.predict(x_val, batch_size = 32, verbose=2)\n\nprint(fbeta_score(y_val, np.array(p_val) > 0.2, beta=2, average='samples')) \n\n        \n\n#to save the test predictions\np_test = cnn_model.predict(x_test, batch_size = 128, verbose=2) \nyfull_test.append(p_test)\n\n","execution_count":20,"outputs":[{"output_type":"stream","text":"253/253 - 1s\n0.8805625770763035\n479/479 - 4s\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#to save the output in a dataframe\n\noutput = np.array(yfull_test[0])\nfor i in range (1,len(yfull_test)):\n    output+=np.array(yfull_test[1])\n    \noutput = pd.DataFrame(output,columns = labels_map)","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output","execution_count":24,"outputs":[{"output_type":"execute_result","execution_count":24,"data":{"text/plain":"       slash_burn     water  agriculture  cultivation    cloudy  \\\n0        0.000734  0.014344     0.008801     0.005340  0.001286   \n1        0.001851  0.019204     0.013353     0.011787  0.001888   \n2        0.001449  0.118012     0.210334     0.048729  0.002603   \n3        0.002254  0.029624     0.041107     0.033083  0.002392   \n4        0.001468  0.108891     0.127817     0.034724  0.085685   \n...           ...       ...          ...          ...       ...   \n61186    0.000665  0.037252     0.034227     0.007276  0.688053   \n61187    0.008250  0.511789     0.333226     0.146601  0.004170   \n61188    0.000964  0.014365     0.010237     0.006310  0.001573   \n61189    0.001009  0.094948     0.016867     0.003968  0.623400   \n61190    0.001726  0.090502     0.093931     0.021344  0.006206   \n\n       artisinal_mine  blow_down  bare_ground   primary  habitation  \\\n0            0.000884   0.001113     0.001143  0.998597    0.001263   \n1            0.001467   0.003185     0.002483  0.998160    0.002391   \n2            0.000842   0.000801     0.002862  0.996257    0.010249   \n3            0.002033   0.003395     0.003671  0.998096    0.004154   \n4            0.001780   0.001406     0.002696  0.917777    0.011657   \n...               ...        ...          ...       ...         ...   \n61186        0.001330   0.000646     0.001181  0.321020    0.003928   \n61187        0.005375   0.004964     0.023149  0.992693    0.044280   \n61188        0.001122   0.001673     0.001494  0.998349    0.001601   \n61189        0.002509   0.001470     0.002722  0.256300    0.003546   \n61190        0.005397   0.002425     0.007752  0.840629    0.780527   \n\n       selective_logging      road  conventional_mine  partly_cloudy  \\\n0               0.002269  0.005343           0.000724       0.001827   \n1               0.007730  0.008895           0.001455       0.003681   \n2               0.001028  0.095491           0.000993       0.998521   \n3               0.006876  0.013663           0.001789       0.017188   \n4               0.001778  0.045866           0.001427       0.953338   \n...                  ...       ...                ...            ...   \n61186           0.001017  0.019505           0.000861       0.289842   \n61187           0.021470  0.167179           0.003534       0.006636   \n61188           0.003299  0.006901           0.000955       0.001895   \n61189           0.000761  0.016607           0.002088       0.004845   \n61190           0.003720  0.889641           0.004261       0.065392   \n\n           haze  blooming     clear  \n0      0.001892  0.009549  0.996408  \n1      0.001526  0.030904  0.996055  \n2      0.001218  0.000988  0.000790  \n3      0.002940  0.019493  0.982609  \n4      0.003121  0.001774  0.001714  \n...         ...       ...       ...  \n61186  0.006665  0.001096  0.001308  \n61187  0.007162  0.011565  0.990224  \n61188  0.001709  0.015232  0.996914  \n61189  0.260396  0.001486  0.046965  \n61190  0.013273  0.003862  0.751187  \n\n[61191 rows x 17 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>slash_burn</th>\n      <th>water</th>\n      <th>agriculture</th>\n      <th>cultivation</th>\n      <th>cloudy</th>\n      <th>artisinal_mine</th>\n      <th>blow_down</th>\n      <th>bare_ground</th>\n      <th>primary</th>\n      <th>habitation</th>\n      <th>selective_logging</th>\n      <th>road</th>\n      <th>conventional_mine</th>\n      <th>partly_cloudy</th>\n      <th>haze</th>\n      <th>blooming</th>\n      <th>clear</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.000734</td>\n      <td>0.014344</td>\n      <td>0.008801</td>\n      <td>0.005340</td>\n      <td>0.001286</td>\n      <td>0.000884</td>\n      <td>0.001113</td>\n      <td>0.001143</td>\n      <td>0.998597</td>\n      <td>0.001263</td>\n      <td>0.002269</td>\n      <td>0.005343</td>\n      <td>0.000724</td>\n      <td>0.001827</td>\n      <td>0.001892</td>\n      <td>0.009549</td>\n      <td>0.996408</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.001851</td>\n      <td>0.019204</td>\n      <td>0.013353</td>\n      <td>0.011787</td>\n      <td>0.001888</td>\n      <td>0.001467</td>\n      <td>0.003185</td>\n      <td>0.002483</td>\n      <td>0.998160</td>\n      <td>0.002391</td>\n      <td>0.007730</td>\n      <td>0.008895</td>\n      <td>0.001455</td>\n      <td>0.003681</td>\n      <td>0.001526</td>\n      <td>0.030904</td>\n      <td>0.996055</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.001449</td>\n      <td>0.118012</td>\n      <td>0.210334</td>\n      <td>0.048729</td>\n      <td>0.002603</td>\n      <td>0.000842</td>\n      <td>0.000801</td>\n      <td>0.002862</td>\n      <td>0.996257</td>\n      <td>0.010249</td>\n      <td>0.001028</td>\n      <td>0.095491</td>\n      <td>0.000993</td>\n      <td>0.998521</td>\n      <td>0.001218</td>\n      <td>0.000988</td>\n      <td>0.000790</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.002254</td>\n      <td>0.029624</td>\n      <td>0.041107</td>\n      <td>0.033083</td>\n      <td>0.002392</td>\n      <td>0.002033</td>\n      <td>0.003395</td>\n      <td>0.003671</td>\n      <td>0.998096</td>\n      <td>0.004154</td>\n      <td>0.006876</td>\n      <td>0.013663</td>\n      <td>0.001789</td>\n      <td>0.017188</td>\n      <td>0.002940</td>\n      <td>0.019493</td>\n      <td>0.982609</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.001468</td>\n      <td>0.108891</td>\n      <td>0.127817</td>\n      <td>0.034724</td>\n      <td>0.085685</td>\n      <td>0.001780</td>\n      <td>0.001406</td>\n      <td>0.002696</td>\n      <td>0.917777</td>\n      <td>0.011657</td>\n      <td>0.001778</td>\n      <td>0.045866</td>\n      <td>0.001427</td>\n      <td>0.953338</td>\n      <td>0.003121</td>\n      <td>0.001774</td>\n      <td>0.001714</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>61186</th>\n      <td>0.000665</td>\n      <td>0.037252</td>\n      <td>0.034227</td>\n      <td>0.007276</td>\n      <td>0.688053</td>\n      <td>0.001330</td>\n      <td>0.000646</td>\n      <td>0.001181</td>\n      <td>0.321020</td>\n      <td>0.003928</td>\n      <td>0.001017</td>\n      <td>0.019505</td>\n      <td>0.000861</td>\n      <td>0.289842</td>\n      <td>0.006665</td>\n      <td>0.001096</td>\n      <td>0.001308</td>\n    </tr>\n    <tr>\n      <th>61187</th>\n      <td>0.008250</td>\n      <td>0.511789</td>\n      <td>0.333226</td>\n      <td>0.146601</td>\n      <td>0.004170</td>\n      <td>0.005375</td>\n      <td>0.004964</td>\n      <td>0.023149</td>\n      <td>0.992693</td>\n      <td>0.044280</td>\n      <td>0.021470</td>\n      <td>0.167179</td>\n      <td>0.003534</td>\n      <td>0.006636</td>\n      <td>0.007162</td>\n      <td>0.011565</td>\n      <td>0.990224</td>\n    </tr>\n    <tr>\n      <th>61188</th>\n      <td>0.000964</td>\n      <td>0.014365</td>\n      <td>0.010237</td>\n      <td>0.006310</td>\n      <td>0.001573</td>\n      <td>0.001122</td>\n      <td>0.001673</td>\n      <td>0.001494</td>\n      <td>0.998349</td>\n      <td>0.001601</td>\n      <td>0.003299</td>\n      <td>0.006901</td>\n      <td>0.000955</td>\n      <td>0.001895</td>\n      <td>0.001709</td>\n      <td>0.015232</td>\n      <td>0.996914</td>\n    </tr>\n    <tr>\n      <th>61189</th>\n      <td>0.001009</td>\n      <td>0.094948</td>\n      <td>0.016867</td>\n      <td>0.003968</td>\n      <td>0.623400</td>\n      <td>0.002509</td>\n      <td>0.001470</td>\n      <td>0.002722</td>\n      <td>0.256300</td>\n      <td>0.003546</td>\n      <td>0.000761</td>\n      <td>0.016607</td>\n      <td>0.002088</td>\n      <td>0.004845</td>\n      <td>0.260396</td>\n      <td>0.001486</td>\n      <td>0.046965</td>\n    </tr>\n    <tr>\n      <th>61190</th>\n      <td>0.001726</td>\n      <td>0.090502</td>\n      <td>0.093931</td>\n      <td>0.021344</td>\n      <td>0.006206</td>\n      <td>0.005397</td>\n      <td>0.002425</td>\n      <td>0.007752</td>\n      <td>0.840629</td>\n      <td>0.780527</td>\n      <td>0.003720</td>\n      <td>0.889641</td>\n      <td>0.004261</td>\n      <td>0.065392</td>\n      <td>0.013273</td>\n      <td>0.003862</td>\n      <td>0.751187</td>\n    </tr>\n  </tbody>\n</table>\n<p>61191 rows × 17 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#to create a list of test predictions\npredt =[]\nfor i in tqdm(range(output.shape[0]), miniters = 1000):\n    a = output.loc[[i]]\n    a = a.apply(lambda  x:x > 0.2,  axis = 1)\n    a = a.transpose()\n    a = a.loc [a[i]  == True]\n    ' '.join(list(a.index))\n    predt.append(' '.join(list(a.index)))","execution_count":27,"outputs":[{"output_type":"stream","text":"100%|██████████| 61191/61191 [02:03<00:00, 494.10it/s]\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#saving the output as a CSV file\nforest_test['tags'] = predt\nforest_test.to_csv('Output.csv', index= False)","execution_count":31,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}